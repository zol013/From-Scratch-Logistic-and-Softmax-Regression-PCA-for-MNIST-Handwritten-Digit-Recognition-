{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"260weyODpQrq"},"outputs":[],"source":["import numpy as np\n","import data\n","import time\n","import tqdm\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HkrP5QkLpQrr"},"outputs":[],"source":["import idx2numpy\n","import numpy as np\n","import os\n","import pickle\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T8oJFUQKpQrr"},"outputs":[],"source":["import argparse\n","import sm_network\n","import data\n","import image\n","import numpy as np\n","from sklearn.decomposition import PCA"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YV_RGaR6pQrs"},"outputs":[],"source":["def load_data(data_directory, train = True):\n","    if train:\n","        images = idx2numpy.convert_from_file(os.path.join(data_directory, 'train-images.idx3-ubyte'))\n","        labels = idx2numpy.convert_from_file(os.path.join(data_directory, 'train-labels.idx1-ubyte'))\n","    else:\n","        images = idx2numpy.convert_from_file(os.path.join(data_directory, 't10k-images.idx3-ubyte'))\n","        labels = idx2numpy.convert_from_file(os.path.join(data_directory, 't10k-labels.idx1-ubyte'))\n","\n","    vdim = images.shape[1] * images.shape[2]\n","    vectors = np.empty([images.shape[0], vdim])\n","    for imnum in range(images.shape[0]):\n","        imvec = images[imnum, :, :].reshape(vdim, 1).squeeze()\n","        vectors[imnum, :] = imvec\n","\n","    return vectors, labels"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YECwoNFXpQrs"},"outputs":[],"source":["def z_score_normalize(X, u = None, sd = None):\n","    \"\"\"\n","    Performs z-score normalization on X.\n","\n","    f(x) = (x - μ) / σ\n","        where\n","            μ = mean of x\n","            σ = standard deviation of x\n","\n","    Parameters\n","    ----------\n","    X : np.array\n","        The data to z-score normalize\n","    u (optional) : np.array\n","        The mean to use when normalizing\n","    sd (optional) : np.array\n","        The standard deviation to use when normalizing\n","\n","    Returns\n","    -------\n","        Tuple:\n","            Transformed dataset with mean 0 and stdev 1\n","            Computed statistics (mean and stdev) for the dataset to undo z-scoring.\n","    \"\"\"\n","\n","    u = np.mean(X, axis = 1)\n","    sd = np.std(X, axis = 1)\n","    X_normalized = np.copy(X)\n","    for i in range(len(X)):\n","        X_normalized[i] = (X[i] - u[i])/sd[i]\n","\n","\n","    return X_normalized, u, sd\n","\n","\n","def min_max_normalize(X, _min = None, _max = None):\n","    \"\"\"\n","    Performs min-max normalization on X.\n","\n","    f(x) = (x - min(x)) / (max(x) - min(x))\n","\n","    Parameters\n","    ----------\n","    X : np.array\n","        The data to min-max normalize\n","    _min (optional) : np.array\n","        The min to use when normalizing\n","    _max (optional) : np.array\n","        The max to use when normalizing\n","\n","    Returns\n","    -------\n","        Tuple:\n","            Transformed dataset with all values in [0,1]\n","            Computed statistics (min and max) for the dataset to undo min-max normalization.\n","    \"\"\"\n","    X_min = np.zeros(len(X))\n","    X_max = np.zeros(len(X))\n","    X_normalized = np.zeros((len(X), len(X[0])))\n","\n","    for i in range(len(X)):\n","        X_min[i] = np.min(X[i])\n","        X_max[i] = np.max(X[i])\n","        X_normalized[i] = (X[i] - X_min[i]) / (X_max[i] - X_min[i])\n","\n","    return X_normalized, X_min, X_max\n","\n","def onehot_encode(y):\n","    \"\"\"\n","    Performs one-hot encoding on y.\n","\n","    Ideas:\n","        NumPy's `eye` function\n","\n","    Parameters\n","    ----------\n","    y : np.array\n","        1d array (length n) of targets (k)\n","\n","    Returns\n","    -------\n","        2d array (shape n*k) with each row corresponding to a one-hot encoded version of the original value.\n","    \"\"\"\n","    diagonals = np.eye(10)\n","    y_encoded = np.zeros((len(y), 10))\n","\n","    for i in range(len(y)):\n","        y_encoded[i] = diagonals[y[i]]\n","\n","    return y_encoded\n","\n","\n","\n","\n","def onehot_decode(y):\n","    \"\"\"\n","    Performs one-hot decoding on y.\n","\n","    Ideas:\n","        NumPy's `argmax` function\n","\n","    Parameters\n","    ----------\n","    y : np.array\n","        2d array (shape n*k) with each row corresponding to a one-hot encoded version of the original value.\n","\n","    Returns\n","    -------\n","        1d array (length n) of targets (k)\n","    \"\"\"\n","    y_decoded = np.argmax(y, axis = 1)\n","\n","    return y_decoded\n","\n","def shuffle(dataset):\n","    \"\"\"\n","    Shuffle dataset.\n","\n","    Make sure that corresponding images and labels are kept together.\n","    Ideas:\n","        NumPy array indexing\n","            https://numpy.org/doc/stable/user/basics.indexing.html#advanced-indexing\n","\n","    Parameters\n","    ----------\n","    dataset\n","        Tuple containing\n","            Images (X)\n","            Labels (y)\n","\n","    Returns\n","    -------\n","        Tuple containing\n","            Images (X)\n","            Labels (y)\n","    \"\"\"\n","    X, y = dataset\n","    indices = np.random.permutation(len(X))\n","    X_shuffled = X[indices]\n","    y_shuffled = y[indices]\n","\n","    return X_shuffled, y_shuffled\n","\n","def append_bias(X):\n","    \"\"\"\n","    Append bias term for dataset.\n","\n","    Parameters\n","    ----------\n","    X\n","        2d numpy array with shape (N,d)\n","\n","    Returns\n","    -------\n","        2d numpy array with shape ((N+1),d)\n","    \"\"\"\n","    X_new = np.insert(X, 0, 1, axis = 1)\n","    return X_new\n","\n","def generate_minibatches(dataset, batch_size=64):\n","    X, y = dataset\n","    l_idx, r_idx = 0, batch_size\n","    while r_idx < len(X):\n","        yield X[l_idx:r_idx], y[l_idx:r_idx]\n","        l_idx, r_idx = r_idx, r_idx + batch_size\n","\n","    yield X[l_idx:], y[l_idx:]\n","\n","def generate_k_fold_set(dataset, k = 5):\n","    X, y = dataset\n","    if k == 1:\n","        yield (X, y), (X[len(X):], y[len(y):])\n","        return\n","\n","    order = np.random.permutation(len(X))\n","\n","    fold_width = len(X) // k\n","\n","    l_idx, r_idx = 0, fold_width\n","\n","    for i in range(k):\n","        train = np.concatenate([X[order[:l_idx]], X[order[r_idx:]]]), np.concatenate([y[order[:l_idx]], y[order[r_idx:]]])\n","        validation = X[order[l_idx:r_idx]], y[order[l_idx:r_idx]]\n","        yield train, validation\n","        l_idx, r_idx = r_idx, r_idx + fold_width"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AJhmaLJlpQru"},"outputs":[],"source":["parser = argparse.ArgumentParser(description = 'CSE151B PA1')\n","parser.add_argument('--batch-size', type = int, default = 512,\n","        help = 'input batch size for training (default: 1)')\n","parser.add_argument('--epochs', type = int, default = 100,\n","        help = 'number of epochs to train (default: 100)')\n","parser.add_argument('--learningrate', type = float, default = 0.01,\n","        help = 'learning rate (default: 0.001)')\n","parser.add_argument('--z-score', dest = 'normalization', action='store_const',\n","        default = data.min_max_normalize, const = data.z_score_normalize,\n","        help = 'use z-score normalization on the dataset, default is min-max normalization')\n","parser.add_argument('--k-folds', type = int, default = 5,\n","        help = 'number of folds for cross-validation')\n","parser.add_argument('--p', type = int, default = 100,\n","        help = 'number of principal components')\n","\n","hyperparameters, unknown = parser.parse_known_args()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z2mQWwvZpQru"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NSxUm0dOpQru"},"outputs":[],"source":["train_images, train_labels = data.load_data('')\n","test_images, test_labels = data.load_data('', train = False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ciDsYnWfpQru"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WQqmlRzNpQrv"},"outputs":[],"source":["train_images, train_labels = data.shuffle((train_images, train_labels))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zQscP3-lpQrv"},"outputs":[],"source":["train_images, train_mean, train_std = data.z_score_normalize(train_images)\n","test_images, test_mean, test_std = data.z_score_normalize(test_images)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M-gCQMoIpQrv"},"outputs":[],"source":["pca = PCA(hyperparameters.p)\n","pca.fit(train_images)\n","train_images = pca.transform(train_images)\n","test_images = pca.transform(test_images)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"78s7LrkupQrv"},"outputs":[],"source":["train_labels = data.onehot_encode(train_labels)\n","test_labels = data.onehot_encode(test_labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"90662814pQrv"},"outputs":[],"source":["train_images = data.append_bias(train_images)\n","test_images = data.append_bias(test_images)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pIn2qHVZpQrv"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mBjjl4z1pQrv"},"outputs":[],"source":["def softmax_regression(hyperparemeters):\n","    network = Network(hyperparameters, softmax, multiclass_cross_entropy, 10)\n","    accuracy = []\n","    val_loss = np.zeros((10,100))\n","    fold_idx = 0\n","    for train_set, val_set in data.generate_k_fold_set((train_images,train_labels), 10):\n","\n","        loss = np.Inf\n","\n","        for epoch in range(100): #hyperparameters.epoch\n","\n","            # stochastic gradient descent on every mini-batch\n","            for minibatch in data.generate_minibatches(train_set, 512):\n","                network.train(minibatch)\n","\n","            # test performance on validation\n","            average_loss, acc = network.test(val_set)\n","            accuracy.append(acc)\n","            val_loss[fold_idx][epoch] = average_loss\n","\n","            if average_loss > loss:\n","                break\n","            loss = average_loss\n","\n","        fold_idx += 1\n","\n","\n","    val_loss = np.sum(val_loss, axis = 0)\n","\n","    for i in range(100):\n","        print(i,'th epoch average validation loss','   ',val_loss[i])\n","\n","\n","    return accuracy\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AD3oe2cppQrv"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ihHiziPopQrv","outputId":"3160ff91-a5c1-437b-85e4-ab5fb2de1923"},"outputs":[{"name":"stdout","output_type":"stream","text":["0 th epoch average validation loss     3.2096920132850375\n","1 th epoch average validation loss     3.1064907419204633\n","2 th epoch average validation loss     0.41435568157292674\n","3 th epoch average validation loss     0.3896866078735272\n","4 th epoch average validation loss     0.37348268253531136\n","5 th epoch average validation loss     0.36187001098168087\n","6 th epoch average validation loss     0.35306542932462404\n","7 th epoch average validation loss     0.3461214394772091\n","8 th epoch average validation loss     0.3404827786787959\n","9 th epoch average validation loss     0.33580001541781185\n","10 th epoch average validation loss     0.33184100456695037\n","11 th epoch average validation loss     0.32844480738527726\n","12 th epoch average validation loss     0.3254959469752722\n","13 th epoch average validation loss     0.3229091841030912\n","14 th epoch average validation loss     0.3206200855706869\n","15 th epoch average validation loss     0.31857895003532966\n","16 th epoch average validation loss     0.316746765445906\n","17 th epoch average validation loss     0.31509244165617406\n","18 th epoch average validation loss     0.31359086904725847\n","19 th epoch average validation loss     0.312221527084474\n","20 th epoch average validation loss     0.3109674679367683\n","21 th epoch average validation loss     0.3098145614160764\n","22 th epoch average validation loss     0.30875092549295863\n","23 th epoch average validation loss     0.30776649087677127\n","24 th epoch average validation loss     0.3068526639584662\n","25 th epoch average validation loss     0.3060020629437211\n","26 th epoch average validation loss     0.305208309149095\n","27 th epoch average validation loss     0.3044658603653892\n","28 th epoch average validation loss     0.3037698766494836\n","29 th epoch average validation loss     0.3031161113643721\n","30 th epoch average validation loss     0.3025008220586109\n","31 th epoch average validation loss     0.3019206970685924\n","32 th epoch average validation loss     0.30137279468037426\n","33 th epoch average validation loss     0.300854492398571\n","34 th epoch average validation loss     0.30036344440499535\n","35 th epoch average validation loss     0.2998975456964305\n","36 th epoch average validation loss     0.29945490170263517\n","37 th epoch average validation loss     0.29903380242655686\n","38 th epoch average validation loss     0.2986327003362689\n","39 th epoch average validation loss     0.29825019138521247\n","40 th epoch average validation loss     0.2978849986534305\n","41 th epoch average validation loss     0.2975359581947307\n","42 th epoch average validation loss     0.2972020067484456\n","43 th epoch average validation loss     0.2968821710337335\n","44 th epoch average validation loss     0.2965755583922705\n","45 th epoch average validation loss     0.2962813485840958\n","46 th epoch average validation loss     0.29599878657314815\n","47 th epoch average validation loss     0.2957271761650806\n","48 th epoch average validation loss     0.2954658743814061\n","49 th epoch average validation loss     0.2952142864717802\n","50 th epoch average validation loss     0.2949718614809729\n","51 th epoch average validation loss     0.29473808829937687\n","52 th epoch average validation loss     0.2945124921361899\n","53 th epoch average validation loss     0.2942946313630468\n","54 th epoch average validation loss     0.29408409468316143\n","55 th epoch average validation loss     0.29388049858719234\n","56 th epoch average validation loss     0.29368348506226827\n","57 th epoch average validation loss     0.29349271952504863\n","58 th epoch average validation loss     0.29330788895348475\n","59 th epoch average validation loss     0.29312870019518805\n","60 th epoch average validation loss     0.2929548784330912\n","61 th epoch average validation loss     0.29278616579148065\n","62 th epoch average validation loss     0.29262232006754124\n","63 th epoch average validation loss     0.2924631135753363\n","64 th epoch average validation loss     0.2923083320906888\n","65 th epoch average validation loss     0.29215777388677516\n","66 th epoch average validation loss     0.2920112488514054\n","67 th epoch average validation loss     0.29186857767798635\n","68 th epoch average validation loss     0.2917295911230524\n","69 th epoch average validation loss     0.29159412932402784\n","70 th epoch average validation loss     0.29146204117157165\n","71 th epoch average validation loss     0.29133318373145445\n","72 th epoch average validation loss     0.2912074217114506\n","73 th epoch average validation loss     0.2910846269691933\n","74 th epoch average validation loss     0.29096467805735743\n","75 th epoch average validation loss     0.29084745980289956\n","76 th epoch average validation loss     0.2907328629174107\n","77 th epoch average validation loss     0.290620783635926\n","78 th epoch average validation loss     0.2905111233817933\n","79 th epoch average validation loss     0.2904037884554312\n","80 th epoch average validation loss     0.2902986897450147\n","81 th epoch average validation loss     0.29019574245730423\n","82 th epoch average validation loss     0.2900948658670058\n","83 th epoch average validation loss     0.28999598308318864\n","84 th epoch average validation loss     0.28989902083142716\n","85 th epoch average validation loss     0.28980390925044497\n","86 th epoch average validation loss     0.28971058170215325\n","87 th epoch average validation loss     0.2896189745940663\n","88 th epoch average validation loss     0.2895290272131674\n","89 th epoch average validation loss     0.2894406815703765\n","90 th epoch average validation loss     0.289353882254841\n","91 th epoch average validation loss     0.2892685762973387\n","92 th epoch average validation loss     0.2891847130421362\n","93 th epoch average validation loss     0.28910224402670287\n","94 th epoch average validation loss     0.28902112286872744\n","95 th epoch average validation loss     0.2889413051599279\n","96 th epoch average validation loss     0.28886274836618625\n","97 th epoch average validation loss     0.2887854117335766\n","98 th epoch average validation loss     0.2887092561998859\n","99 th epoch average validation loss     0.2886342443112623\n"]}],"source":["accuracy = softmax_regression(hyperparameters)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"njv03fBPpQrw"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hd2o5MyzpQrw"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i0q79Jq1pQrw"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XZFJzGKRpQrw"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P6114TwjpQrw"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6KaEihsipQrw"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pp3wSUMlpQrw"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZhH3soXmpQrx"},"outputs":[],"source":["def sigmoid(a):\n","    \"\"\"\n","    Compute the sigmoid function.\n","\n","    f(x) = 1 / (1 + e ^ (-x))\n","\n","    Parameters\n","    ----------\n","    a\n","        The internal value while a pattern goes through the network\n","    Returns\n","    -------\n","    float\n","       Value after applying sigmoid (z from the slides).\n","    \"\"\"\n","    return 1/(1 + np.exp(-a))\n","\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HPIJH9b2pQrx"},"outputs":[],"source":["b"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zy6Ry1ePpQrx"},"outputs":[],"source":["def softmax(a):\n","    \"\"\"\n","    Compute the softmax function.\n","\n","    f(x) = (e^x) / Σ (e^x)\n","\n","    Parameters\n","    ----------\n","    a\n","        The internal value while a pattern goes through the network\n","    Returns\n","    -------\n","    float\n","       Value after applying softmax (z from the slides).\n","    \"\"\"\n","    x = np.exp(a)\n","    return x/np.sum(x, axis = 1, keepdims = True)\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BYj7zXLmpQrx"},"outputs":[],"source":["def binary_cross_entropy(y, t):\n","    \"\"\"\n","    Compute binary cross entropy.\n","\n","    L(x) = t*ln(y) + (1-t)*ln(1-y)\n","\n","    Parameters\n","    ----------\n","    y\n","        The network's predictions\n","    t\n","        The corresponding targets\n","    Returns\n","    -------\n","    float\n","        binary cross entropy loss value according to above definition\n","    \"\"\"\n","    pass\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"12j5HiRipQrx"},"outputs":[],"source":["def multiclass_cross_entropy(y, t):\n","    \"\"\"\n","    Compute multiclass cross entropy.\n","\n","    L(x) = - Σ (t*ln(y))\n","\n","    Parameters\n","    ----------\n","    y\n","        The network's predictions\n","    t\n","        The corresponding targets\n","    Returns\n","    -------\n","    float\n","        multiclass cross entropy loss value according to above definition\n","    \"\"\"\n","    entropy = np.multiply(np.log(y), t)\n","\n","    return - np.sum(entropy)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cYnUwSdgpQrx"},"outputs":[],"source":["class Network:\n","    def __init__(self, hyperparameters, activation, loss, out_dim):\n","        \"\"\"\n","        Perform required setup for the network.\n","\n","        Initialize the weight matrix, set the activation function, save hyperparameters.\n","\n","        You may want to create arrays to save the loss values during training.\n","\n","        Parameters\n","        ----------\n","        hyperparameters\n","            A Namespace object from `argparse` containing the hyperparameters\n","        activation\n","            The non-linear activation function to use for the network\n","        loss\n","            The loss function to use while training and testing\n","        \"\"\"\n","        self.hyperparameters = hyperparameters\n","        self.activation = activation\n","        self.loss = loss\n","\n","        self.weights = np.zeros((hyperparameters.p + 1, out_dim))\n","\n","    def forward(self, X):\n","        \"\"\"\n","        Apply the model to the given patterns\n","\n","        Use `self.weights` and `self.activation` to compute the network's output\n","\n","        f(x) = σ(w*x)\n","            where\n","                σ = non-linear activation function\n","                w = weight matrix\n","\n","        Make sure you are using matrix multiplication when you vectorize your code!\n","\n","        Parameters\n","        ----------\n","        X\n","            Patterns to create outputs for\n","        \"\"\"\n","        return self.activation(X.dot(self.weights))\n","\n","\n","\n","    def __call__(self, X):\n","        return self.forward(X)\n","\n","    def train(self, minibatch):\n","        \"\"\"\n","        Train the network on the given minibatch\n","\n","        Use `self.weights` and `self.activation` to compute the network's output\n","        Use `self.loss` and the gradient defined in the slides to update the network.\n","\n","        Parameters\n","        ----------\n","        minibatch\n","            The minibatch to iterate over\n","\n","        Returns\n","        -------\n","        tuple containing:\n","            average loss over minibatch\n","            accuracy over minibatch\n","        \"\"\"\n","        train_images, train_labels = minibatch\n","        B = len(train_labels)\n","        sm_scores = self.activation(train_images @ self.weights)\n","        #avarage_loss = self.loss(sm_scores, train_labels) / B\n","        predict_labels = np.argmax(sm_scores, axis = 1)\n","        #accuracy = sum(predict_labels == train_labels)/B\n","        gradient = train_images.T @ (train_labels - sm_scores)\n","        self.weights += hyperparameters.learningrate * gradient/ B\n","\n","        #return (avarage_loss,accuracy)\n","\n","    def test(self, minibatch):\n","        \"\"\"\n","        Test the network on the given minibatch\n","\n","        Use `self.weights` and `self.activation` to compute the network's output\n","        Use `self.loss` to compute the loss.\n","        Do NOT update the weights in this method!\n","\n","        Parameters\n","        ----------\n","        minibatch\n","            The minibatch to iterate over\n","\n","        Returns\n","        -------\n","            tuple containing:\n","                average loss over minibatch\n","                accuracy over minibatch\n","        \"\"\"\n","        test_images, test_labels = minibatch\n","        B = len(test_labels)\n","        sm_scores = self.activation(test_images @ self.weights)\n","        decoded_test_labels = onehot_decode(test_labels)\n","        avarage_loss = self.loss(sm_scores, test_labels) / B\n","        predict_labels = np.argmax(sm_scores, axis = 1)\n","\n","\n","        accuracy = sum(predict_labels == decoded_test_labels)/B\n","\n","        return (avarage_loss,accuracy)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wQm5-sx1pQrx"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rNDtD1xkpQry"},"outputs":[],"source":["    def forward(self, X):\n","        \"\"\"\n","        Apply the model to the given patterns\n","\n","        Use `self.weights` and `self.activation` to compute the network's output\n","\n","        f(x) = σ(w*x)\n","            where\n","                σ = non-linear activation function\n","                w = weight matrix\n","\n","        Make sure you are using matrix multiplication when you vectorize your code!\n","\n","        Parameters\n","        ----------\n","        X\n","            Patterns to create outputs for\n","        \"\"\"\n","        return self.activation(X.dot(self.weights))\n","\n","\n","\n","    def __call__(self, X):\n","        return self.forward(X)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w8679TM1pQry"},"outputs":[],"source":["    def train(self, minibatch):\n","        \"\"\"\n","        Train the network on the given minibatch\n","\n","        Use `self.weights` and `self.activation` to compute the network's output\n","        Use `self.loss` and the gradient defined in the slides to update the network.\n","\n","        Parameters\n","        ----------\n","        minibatch\n","            The minibatch to iterate over\n","\n","        Returns\n","        -------\n","        tuple containing:\n","            average loss over minibatch\n","            accuracy over minibatch\n","        \"\"\"\n","        train_images, train_labels = minibatch\n","        B = len(train_labels)\n","        sm_scores = self.activation(train_images @ self.weights)\n","        #avarage_loss = self.loss(sm_scores, train_labels) / B\n","        predict_labels = np.argmax(sm_scores, axis = 1)\n","        #accuracy = sum(predict_labels == train_labels)/B\n","        gradient = X.T @ (train_images - sm_scores)\n","        self.weights -= hyperparameters.learningrate * gradient/ B\n","\n","        #return (avarage_loss,accuracy)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DaJTfCt5pQry"},"outputs":[],"source":["a"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h1kS4L-dpQry"},"outputs":[],"source":["    def test(self, minibatch):\n","        \"\"\"\n","        Test the network on the given minibatch\n","\n","        Use `self.weights` and `self.activation` to compute the network's output\n","        Use `self.loss` to compute the loss.\n","        Do NOT update the weights in this method!\n","\n","        Parameters\n","        ----------\n","        minibatch\n","            The minibatch to iterate over\n","\n","        Returns\n","        -------\n","            tuple containing:\n","                average loss over minibatch\n","                accuracy over minibatch\n","        \"\"\"\n","        test_images, test_labels = minibatch\n","        B = len(test_labels)\n","        sm_scores = self.activation(test_images @ self.weights)\n","        avarage_loss = self.loss(sm_scores, test_labels) / B\n","        predict_labels = np.argmax(sm_scores, axis = 1)\n","\n","        accuracy = sum(predict_labels == test_labels)/B\n","\n","        return (avarage_loss,accuracy)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}